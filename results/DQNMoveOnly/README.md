# DQNMoveOnly Results

## Model Details

### Inputs/Outputs
This model operates on reduced state and action spaces, processing only the `player_relative` screen feature layer, and outputting a spacial coordinate meant to be used as an argument to `Move_screen("now", ...)`.

### Estimator
A deep Q network (DQN) is used to model the value (Q) of each action conditioned on the state. The value represents the expected total reward due to selecting an action, and is equal to the expected immediate reward plus the best value achievable from the state the environment will transition to as a result of the action - discounted multiplicatively by `discount_factor`.

### Policy
At each step, the action having the maximum Q-value according to the DQN is chosen.

### Training Procedure
An epsilon-greedy strategy is used to explore the state/action space. The probability of selecting a random action, epsilon, is annealed linearly from a maximum value `epsilon_max` to a minimum value `epsilon_min` over a number of steps equaling `epsilon_decay_steps`. When selecting actions nonrandomly, the agent acts according to its policy, selecting the action with the highest estimated value.

After every `train_frequency` steps, the online DQN (the network used to inform the policy) has its weights updated using gradient descent, where the target Q given an action is the immediate reward plus the maximum estimated Q in the resulting state.

A separate copy of the network is used to calculate the values of states used as a term in the target Q's when training the online DQN. Periodically (every `target_update_frequency` steps), the weights of the online DQN are copied over to the target DQN. This is to stabilize the values predicted, and to prevent feedback loops leading to large overestimations.

The action-reward-next_state data used as inputs during the gradient updates to the online DQN are stored in a memory buffer. During training, a random sample of `batch_size` experiences are sampled from the buffer. This Experience Replay method is meant to decorrelate the experiences the value estimator trains on, since those generated by the training (epsilon greedy) policy will be highly autocorrelated. The Experience Replay buffer stores a maximum of `max_memory` experiences in a double-ended queue.

## Results
<table align="center">
  <tr>
    <td align="center"></td>
    <td align="center">Mean Score</td>
    <td align="center">Max Score</td>
    <td align="center">Training Episodes</td>
    <td align="center">Hyperparameters</td>
    <td align="center">Checkpoint</td>
    <td align="center">Notes</td>

  </tr>
  <tr>
    <td align="center">MoveToBeacon</td>
    <td align="center"></td>
    <td align="center"></td>
    <td align="center"></td>
    <td align="center">`learning_rate`: 1e-5\n
                       `discount_factor`: 0.95\n
                       `epsilon_max`: 1.0\n
                       `epsilon_min`: 0.01\n
                       `epsilon_decay_steps`: 10,000\n
                       `train_frequency`> 1\n
                       `target_update_frequency`: 500\n
                       `max_memory`: 10000\n
                       `batch_size`: 16
                     </td>
    <td align="center"></td>
    <td align="center">`step_mul` flag set to 16 during training</td>
  </tr>
  <tr>
    <td align="center">CollectMineralShards</td>
    <td align="center"></td>
    <td align="center"></td>
    <td align="center"></td>
    <td align="center"></td>
    <td align="center"></td>
    <td align="center"></td>
  </tr>
</table>

## Training Notes/Caveats
* The Experience Replay buffer does not persist over multiple runs, and is repopulated from scratch each time.
* The target network is updated at the beginning of each new run, even when restoring the online network from a checkpoint.
* Tensorboard summaries are written at the beginning of each episode, for the previous episode. The last episode of a run is not summarized.